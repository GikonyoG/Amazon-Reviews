{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2be03ca2-4bb6-497a-85ee-8e7b3ff07e85",
   "metadata": {},
   "source": [
    "\n",
    "# Natural Language Processing with PyTorch:  \n",
    "# Building and Optimizing Sentiment Analysis Models Using Amazon Reviews\n",
    "\n",
    "\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This tutorial demonstrates how to build, train, and compare multiple sentiment analysis models using PyTorch and the Amazon Reviews dataset (amazon_polarity from Hugging Face). The guide covers data loading and preprocessing, model architecture design (BoW+MLP, LSTM, CNN, LSTM with Attention), hyperparameter experimentation, and detailed evaluation using graphs and tables. Critical comparisons with similar tutorials are provided to highlight novel insights and improvements. Following this tutorial, you will understand end-to-end NLP model development with real-world data.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the structure and contents of the Amazon Reviews dataset.\n",
    "- Preprocess text data and build a vocabulary.\n",
    "- Implement multiple neural network models in PyTorch for sentiment analysis.\n",
    "- Compare models and analyze hyperparameter effects on performance.\n",
    "- Visualize results using graphs and tables and interpret model outputs (e.g., attention visualization).\n",
    "- Critically evaluate the approach against existing tutorials.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Dataset Overview](#dataset-overview)\n",
    "3. [Data Preprocessing and Vocabulary Building](#data-preprocessing)\n",
    "4. [Model Architectures](#model-architectures)\n",
    "   - 4.1 [Bag-of-Words + MLP](#bow-model)\n",
    "   - 4.2 [LSTM Model](#lstm-model)\n",
    "   - 4.3 [CNN Model](#cnn-model)\n",
    "   - 4.4 [LSTM with Attention](#attn-model)\n",
    "5. [Training Process](#training-process)\n",
    "6. [Hyperparameter Experimentation](#hyperparameter-experimentation)\n",
    "7. [Results and Evaluation](#results)\n",
    "   - 7.1 [Visualization of Learning Curves, ROC, etc.](#results-visualization)\n",
    "   - 7.2 [Model Comparison](#model-comparison)\n",
    "8. [Conclusion](#conclusion)\n",
    "9. [References](#references)\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "In this tutorial, we develop a sentiment analysis pipeline using Amazon product reviews. We explore various deep learning models—from a simple bag-of-words approach to more complex architectures such as LSTM, CNN, and LSTM with Attention. We will preprocess the raw text data, build numerical representations, and train the models using PyTorch. In addition, we’ll examine how hyperparameter configurations affect model performance. Detailed visualizations and critical comparisons with existing tutorials are included.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0ecdb4-ce88-4001-9d7d-3ca1e5d20342",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview\n",
    "\n",
    "We use the [amazon_polarity](https://huggingface.co/datasets/amazon_polarity) dataset from Hugging Face. It contains:\n",
    "- **Train Split:** 3,600,000 examples\n",
    "- **Test Split:** 400,000 examples\n",
    "- **Features:**\n",
    "  - `label`: Sentiment (0 for negative, 1 for positive)\n",
    "  - `title`: Review title\n",
    "  - `content`: Full review text\n",
    "\n",
    "For faster processing, we select a smaller subset:\n",
    "- 100,000 examples for training\n",
    "- 10,000 examples for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1965dbbd-3b94-4f73-94eb-e3e4894ad117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "# Load the dataset from Hugging Face\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86b1fb1-0eda-41ec-9e23-08c44c804ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('amazon_polarity')\n",
    "small_train = dataset['train'].select(range(100000))\n",
    "small_test = dataset['test'].select(range(10000))\n",
    "print(\"Sample training record:\")\n",
    "print(small_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df413c49-0429-4509-89fe-b4eae82739db",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Vocabulary Building\n",
    "\n",
    "We clean the text (convert to lowercase, remove punctuation and extra whitespace) and build a vocabulary using the training subset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98d4360-8493-4a9d-82bb-871229f2f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text data.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def build_vocab(dataset_split, min_freq=5, max_words=50000):\n",
    "    \"\"\"\n",
    "    Build vocabulary from dataset (iterating by index).\n",
    "    \n",
    "    Args:\n",
    "        dataset_split: Dataset to build vocabulary from.\n",
    "        min_freq: Minimum frequency for a word to be included.\n",
    "        max_words: Maximum vocabulary size.\n",
    "        \n",
    "    Returns:\n",
    "        vocab: Dictionary mapping words to indices.\n",
    "    \"\"\"\n",
    "    print(f\"Building vocabulary (min_freq={min_freq}, max_words={max_words})...\")\n",
    "    counter = Counter()\n",
    "    for i in tqdm(range(len(dataset_split)), desc=\"Building vocab\"):\n",
    "        example = dataset_split[i]\n",
    "        tokens = clean_text(example['content']).split()\n",
    "        counter.update(tokens)\n",
    "    words = [word for word, freq in counter.most_common(max_words) if freq >= min_freq]\n",
    "    vocab = {word: idx+2 for idx, word in enumerate(words)}\n",
    "    vocab[\"<PAD>\"] = 0\n",
    "    vocab[\"<UNK>\"] = 1\n",
    "    print(f\"Vocabulary built: {len(vocab)} words\")\n",
    "    return vocab\n",
    "\n",
    "# Build vocabulary from the training subset\n",
    "vocab = build_vocab(small_train, min_freq=5)\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "\n",
    "def precompute_sequences(dataset_split, vocab, max_len=100):\n",
    "    \"\"\"\n",
    "    Precompute sequences from dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset_split: Dataset to process.\n",
    "        vocab: Vocabulary dictionary.\n",
    "        max_len: Maximum sequence length.\n",
    "        \n",
    "    Returns:\n",
    "        sequences: List of token index sequences.\n",
    "        labels: List of labels.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in tqdm(range(len(dataset_split)), desc=\"Precomputing sequences\"):\n",
    "        example = dataset_split[i]\n",
    "        tokens = clean_text(example['content']).split()\n",
    "        seq = [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n",
    "        if len(seq) < max_len:\n",
    "            seq = seq + [vocab[\"<PAD>\"]] * (max_len - len(seq))\n",
    "        else:\n",
    "            seq = seq[:max_len]\n",
    "        sequences.append(seq)\n",
    "        labels.append(example['label'])\n",
    "    return sequences, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1b6857-cc06-43f8-9ff9-707f81418d19",
   "metadata": {},
   "source": [
    "## 4. Model Architectures\n",
    "\n",
    "We implement four models:\n",
    "- **Bag-of-Words + MLP**\n",
    "- **LSTM Model**\n",
    "- **CNN Model**\n",
    "- **LSTM with Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0b355e-3d22-4af7-b752-1e32d68a9637",
   "metadata": {},
   "source": [
    "### 4.1 Bag-of-Words + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ba4f3-8d1b-4d04-8d66-8645b0ac4a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, dropout=0.2):\n",
    "        super(BoWModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # [batch, max_len, embed_dim]\n",
    "        avg_embedded = embedded.mean(dim=1)  # Mean pooling\n",
    "        hidden = F.relu(self.fc1(avg_embedded))\n",
    "        hidden = self.dropout(hidden)\n",
    "        logits = self.fc2(hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c0c7b1-283e-450c-b3c7-2504450d04f7",
   "metadata": {},
   "source": [
    "### 4.2 LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15e4648-3754-4649-ada7-94faa6181a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, num_classes, bidirectional=True, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers,\n",
    "                            batch_first=True, bidirectional=bidirectional,\n",
    "                            dropout=dropout if num_layers > 1 else 0)\n",
    "        lstm_out_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(lstm_out_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lengths = (x != 0).sum(dim=1).cpu()\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (h_n, _) = self.lstm(packed_embedded)\n",
    "        if self.lstm.bidirectional:\n",
    "            h = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        else:\n",
    "            h = h_n[-1]\n",
    "        h = self.dropout(h)\n",
    "        logits = self.fc(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19200707-92d2-4e8d-aa4a-da0af72c323e",
   "metadata": {},
   "source": [
    "### 4.3 CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc9d65-6421-491e-a83a-999ec41452c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes, kernel_sizes=[3,4,5], num_filters=100, dropout=0.2):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embed_dim, out_channels=num_filters, kernel_size=k)\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_filters * len(kernel_sizes), num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        conv_outs = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(out, kernel_size=out.size(2)).squeeze(2) for out in conv_outs]\n",
    "        concat = torch.cat(pooled, dim=1)\n",
    "        concat = self.dropout(concat)\n",
    "        logits = self.fc(concat)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ba3b25-e683-497f-b1f6-6b9102738e21",
   "metadata": {},
   "source": [
    "### 4.4 LSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2784dc-a4c9-4f69-b782-7701e0db3055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, num_classes, bidirectional=True, dropout=0.2):\n",
    "        super(AttnLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers,\n",
    "                           batch_first=True, bidirectional=bidirectional,\n",
    "                           dropout=dropout if num_layers > 1 else 0)\n",
    "        self.lstm_out_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        self.attention = nn.Linear(self.lstm_out_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(self.lstm_out_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        mask = (x != 0).float()\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        attn_weights = self.attention(lstm_out).squeeze(-1)\n",
    "        attn_weights = attn_weights.masked_fill(mask == 0, -1e10)\n",
    "        attn_weights = F.softmax(attn_weights, dim=1).unsqueeze(2)\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "        context = self.dropout(context)\n",
    "        logits = self.fc(context)\n",
    "        return logits, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be005f18-8f94-4a31-93ce-f4d2bbb70533",
   "metadata": {},
   "source": [
    "## 5. Creating a Dataset and DataLoader\n",
    "\n",
    "We precompute sequences for training and testing, then create Dataset objects and DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dfb6f4-2c1a-40e3-94c8-5dd4fe17affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute sequences for training and testing\n",
    "max_seq_len = 50  # Shorter sequences for faster training\n",
    "train_sequences, train_labels = precompute_sequences(small_train, vocab, max_len=max_seq_len)\n",
    "test_sequences, test_labels = precompute_sequences(small_test, vocab, max_len=max_seq_len)\n",
    "\n",
    "# Define a dataset class\n",
    "class AmazonReviewsDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "train_dataset = AmazonReviewsDataset(train_sequences, train_labels)\n",
    "test_dataset = AmazonReviewsDataset(test_sequences, test_labels)\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 0  # Set to 0 to avoid multiprocessing issues\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1cc739-7e5f-4012-9551-fd472a43a260",
   "metadata": {},
   "source": [
    "## 6. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf22bed-2ba1-44a2-b773-bf470757498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(train_losses, val_losses, model_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
    "    plt.title(f'Learning Curves - {model_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{model_name}_learning_curves.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(true_labels, predictions, model_name, classes=['Negative', 'Positive']):\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(f'{model_name}_confusion_matrix.png', dpi=300)\n",
    "    plt.show()\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    print(f\"Metrics for {model_name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "def plot_roc_curve(true_labels, prediction_probs, model_name):\n",
    "    fpr, tpr, _ = roc_curve(true_labels, prediction_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {model_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{model_name}_roc_curve.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall_curve(true_labels, prediction_probs, model_name):\n",
    "    precision, recall, _ = precision_recall_curve(true_labels, prediction_probs)\n",
    "    avg_precision = np.mean(precision)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, color='blue', lw=2, label=f'Precision-Recall curve (AP = {avg_precision:.3f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title(f'Precision-Recall Curve - {model_name}')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{model_name}_pr_curve.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=5, lr=0.001, device='cpu', use_attention=False):\n",
    "    device = torch.device(device)\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    best_val_acc = 0.0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\"):\n",
    "            features, labels = batch[0].to(device), batch[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            if use_attention:\n",
    "                outputs, _ = model(features)\n",
    "            else:\n",
    "                outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * features.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        val_probs = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\"):\n",
    "                features, labels = batch[0].to(device), batch[1].to(device)\n",
    "                if use_attention:\n",
    "                    outputs, _ = model(features)\n",
    "                else:\n",
    "                    outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * features.size(0)\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                val_probs.extend(probs[:, 1].cpu().numpy())\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            model_name_str = model.__class__.__name__\n",
    "            torch.save(model.state_dict(), f\"best_{model_name_str.lower()}_model.pt\")\n",
    "            print(f\"  New best model saved with val acc: {val_acc:.4f}\")\n",
    "    model_name_str = model.__class__.__name__\n",
    "    plot_learning_curves(train_losses, val_losses, model_name_str)\n",
    "    model.load_state_dict(torch.load(f\"best_{model_name_str.lower()}_model.pt\"))\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate_model(model, test_loader, device='cpu', use_attention=False):\n",
    "    device = torch.device(device)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            features, labels = batch[0].to(device), batch[1].to(device)\n",
    "            if use_attention:\n",
    "                outputs, _ = model(features)\n",
    "            else:\n",
    "                outputs = model(features)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "    accuracy_val = accuracy_score(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, target_names=[\"Negative\", \"Positive\"], output_dict=True)\n",
    "    model_name_str = model.__class__.__name__\n",
    "    plot_confusion_matrix(all_labels, all_preds, f\"{model_name_str} (Test)\")\n",
    "    plot_roc_curve(all_labels, all_probs, f\"{model_name_str} (Test)\")\n",
    "    plot_precision_recall_curve(all_labels, all_probs, f\"{model_name_str} (Test)\")\n",
    "    print(f\"\\nTest Results for {model_name_str}:\")\n",
    "    print(f\"Accuracy: {accuracy_val:.4f}\")\n",
    "    print(f\"Precision (Positive): {report['Positive']['precision']:.4f}\")\n",
    "    print(f\"Recall (Positive): {report['Positive']['recall']:.4f}\")\n",
    "    print(f\"F1-Score (Positive): {report['Positive']['f1-score']:.4f}\")\n",
    "    results = {\n",
    "        'accuracy': accuracy_val,\n",
    "        'precision': report['Positive']['precision'],\n",
    "        'recall': report['Positive']['recall'],\n",
    "        'f1': report['Positive']['f1-score'],\n",
    "        'predictions': all_preds,\n",
    "        'true_labels': all_labels,\n",
    "        'probabilities': all_probs\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e55172-465e-4d24-9159-518d408bd686",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1420d0f-36a6-4e03-b74e-189d259e0adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_quick(model, train_loader, val_loader, num_epochs=2, learning_rate=1e-3, device='cpu', use_attention=False):\n",
    "    \"\"\"A faster version for hyperparameter experiments.\"\"\"\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (texts, labels) in enumerate(train_loader):\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            if use_attention:\n",
    "                outputs, _ = model(texts)\n",
    "            else:\n",
    "                outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            if batch_idx >= 20:\n",
    "                break\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (texts, labels) in enumerate(val_loader):\n",
    "                texts, labels = texts.to(device), labels.to(device)\n",
    "                if use_attention:\n",
    "                    outputs, _ = model(texts)\n",
    "                else:\n",
    "                    outputs = model(texts)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                if batch_idx >= 10:\n",
    "                    break\n",
    "    return model\n",
    "\n",
    "def evaluate_model_quick(model, test_loader, device='cpu', use_attention=False):\n",
    "    \"\"\"Evaluate model accuracy on a subset for fast experiments.\"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (texts, labels) in enumerate(test_loader):\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            if use_attention:\n",
    "                outputs, _ = model(texts)\n",
    "            else:\n",
    "                outputs = model(texts)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            if batch_idx >= 20:\n",
    "                break\n",
    "    accuracy_val = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Test Accuracy (on subset): {accuracy_val:.4f}\")\n",
    "    return accuracy_val\n",
    "\n",
    "def run_hyperparameter_experiments(train_dataset, test_dataset, vocab_size, device='cpu'):\n",
    "    print(\"\\n==== Running Hyperparameter Experiments ====\\n\")\n",
    "    results_data = []\n",
    "    max_epochs = 2\n",
    "    test_loader_exp = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "    print(\"\\n--- Experiment 1: Effect of Embedding Dimension ---\")\n",
    "    embedding_dims = [16, 32, 64, 128]\n",
    "    for embed_dim in embedding_dims:\n",
    "        print(f\"\\nTesting embedding dimension: {embed_dim}\")\n",
    "        model = BoWModel(vocab_size, embed_dim, hidden_dim=64, num_classes=2, dropout=0.2)\n",
    "        batch_size = 128\n",
    "        train_loader_exp = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        start_time = time.time()\n",
    "        train_model_quick(model, train_loader_exp, test_loader_exp, num_epochs=max_epochs, learning_rate=0.001, device=device)\n",
    "        training_time = time.time() - start_time\n",
    "        val_accuracy = evaluate_model_quick(model, test_loader_exp, device)\n",
    "        results_data.append({\n",
    "            'model_type': 'BoW',\n",
    "            'embedding_dim': embed_dim,\n",
    "            'hidden_dim': 64,\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': batch_size,\n",
    "            'dropout': 0.2,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'training_time': training_time\n",
    "        })\n",
    "    print(\"\\n--- Experiment 2: Effect of Learning Rate ---\")\n",
    "    learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\nTesting learning rate: {lr}\")\n",
    "        model = CNNModel(vocab_size, embed_dim=64, num_classes=2, dropout=0.2)\n",
    "        batch_size = 128\n",
    "        train_loader_exp = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        start_time = time.time()\n",
    "        train_model_quick(model, train_loader_exp, test_loader_exp, num_epochs=max_epochs, learning_rate=lr, device=device)\n",
    "        training_time = time.time() - start_time\n",
    "        val_accuracy = evaluate_model_quick(model, test_loader_exp, device)\n",
    "        results_data.append({\n",
    "            'model_type': 'CNN',\n",
    "            'embedding_dim': 64,\n",
    "            'hidden_dim': None,\n",
    "            'learning_rate': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'dropout': 0.2,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'training_time': training_time\n",
    "        })\n",
    "    print(\"\\n--- Experiment 3: Effect of Dropout Rate ---\")\n",
    "    dropout_rates = [0.0, 0.1, 0.2, 0.3, 0.5]\n",
    "    for dropout in dropout_rates:\n",
    "        print(f\"\\nTesting dropout rate: {dropout}\")\n",
    "        model = LSTMModel(vocab_size, embed_dim=64, hidden_dim=64, num_layers=1, num_classes=2, dropout=dropout)\n",
    "        batch_size = 128\n",
    "        train_loader_exp = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        start_time = time.time()\n",
    "        train_model_quick(model, train_loader_exp, test_loader_exp, num_epochs=max_epochs, learning_rate=0.001, device=device)\n",
    "        training_time = time.time() - start_time\n",
    "        val_accuracy = evaluate_model_quick(model, test_loader_exp, device)\n",
    "        results_data.append({\n",
    "            'model_type': 'LSTM',\n",
    "            'embedding_dim': 64,\n",
    "            'hidden_dim': 64,\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': batch_size,\n",
    "            'dropout': dropout,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'training_time': training_time\n",
    "        })\n",
    "    print(\"\\n--- Experiment 4: Effect of Hidden Layer Size ---\")\n",
    "    hidden_dims = [32, 64, 128, 256]\n",
    "    for hidden_dim in hidden_dims:\n",
    "        print(f\"\\nTesting hidden dimension: {hidden_dim}\")\n",
    "        model = AttnLSTMModel(vocab_size, embed_dim=64, hidden_dim=hidden_dim, num_layers=1, num_classes=2, dropout=0.2)\n",
    "        batch_size = 128\n",
    "        train_loader_exp = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        start_time = time.time()\n",
    "        train_model_quick(model, train_loader_exp, test_loader_exp, num_epochs=max_epochs, learning_rate=0.001, device=device, use_attention=True)\n",
    "        training_time = time.time() - start_time\n",
    "        val_accuracy = evaluate_model_quick(model, test_loader_exp, device, use_attention=True)\n",
    "        results_data.append({\n",
    "            'model_type': 'LSTM+Attention',\n",
    "            'embedding_dim': 64,\n",
    "            'hidden_dim': hidden_dim,\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': batch_size,\n",
    "            'dropout': 0.2,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'training_time': training_time\n",
    "        })\n",
    "    print(\"\\n--- Experiment 5: Comparing Model Architectures ---\")\n",
    "    models = {\n",
    "        'BoW': BoWModel(vocab_size, embed_dim=64, hidden_dim=64, num_classes=2, dropout=0.2),\n",
    "        'LSTM': LSTMModel(vocab_size, embed_dim=64, hidden_dim=64, num_layers=1, num_classes=2, dropout=0.2),\n",
    "        'CNN': CNNModel(vocab_size, embed_dim=64, num_classes=2, dropout=0.2),\n",
    "        'LSTM+Attention': AttnLSTMModel(vocab_size, embed_dim=64, hidden_dim=64, num_layers=1, num_classes=2, dropout=0.2)\n",
    "    }\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nTraining {model_name} model with best hyperparameters\")\n",
    "        batch_size = 128\n",
    "        train_loader_exp = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        start_time = time.time()\n",
    "        use_attn = model_name == 'LSTM+Attention'\n",
    "        train_model_quick(model, train_loader_exp, test_loader_exp, num_epochs=max_epochs, learning_rate=0.001, device=device, use_attention=use_attn)\n",
    "        training_time = time.time() - start_time\n",
    "        val_accuracy = evaluate_model_quick(model, test_loader_exp, device, use_attention=use_attn)\n",
    "        results_data.append({\n",
    "            'model_type': model_name,\n",
    "            'embedding_dim': 64,\n",
    "            'hidden_dim': 64 if model_name != 'CNN' else None,\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': batch_size,\n",
    "            'dropout': 0.2,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'training_time': training_time\n",
    "        })\n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    return results_df\n",
    "\n",
    "def visualize_hyperparameter_results(results_df):\n",
    "    plt.figure(figsize=(20, 24))\n",
    "    plt.subplot(4, 2, 1)\n",
    "    bow_results = results_df[results_df['model_type'] == 'BoW']\n",
    "    plt.plot(bow_results['embedding_dim'], bow_results['val_accuracy'], 'bo-', linewidth=2, markersize=10)\n",
    "    for x, y in zip(bow_results['embedding_dim'], bow_results['val_accuracy']):\n",
    "        plt.annotate(f'{y:.3f}', (x, y), textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=9, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    plt.xlabel('Embedding Dimension', fontsize=12)\n",
    "    plt.ylabel('Validation Accuracy', fontsize=12)\n",
    "    plt.title('Effect of Embedding Dimension on Accuracy (BoW Model)', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.xticks(bow_results['embedding_dim'])\n",
    "    \n",
    "    plt.subplot(4, 2, 2)\n",
    "    bars = plt.bar(bow_results['embedding_dim'], bow_results['training_time'], width=0.5, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1, f'{height:.1f}s', ha='center', va='bottom', fontsize=9)\n",
    "    plt.xlabel('Embedding Dimension', fontsize=12)\n",
    "    plt.ylabel('Training Time (seconds)', fontsize=12)\n",
    "    plt.title('Effect of Embedding Dimension on Training Time (BoW Model)', fontsize=14)\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(bow_results['embedding_dim'])\n",
    "    \n",
    "    plt.subplot(4, 2, 3)\n",
    "    cnn_results = results_df[results_df['model_type'] == 'CNN']\n",
    "    plt.semilogx(cnn_results['learning_rate'], cnn_results['val_accuracy'], 'ro-', linewidth=2, markersize=10)\n",
    "    for x, y in zip(cnn_results['learning_rate'], cnn_results['val_accuracy']):\n",
    "        plt.annotate(f'{y:.3f}', (x, y), textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=9, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    plt.xlabel('Learning Rate (log scale)', fontsize=12)\n",
    "    plt.ylabel('Validation Accuracy', fontsize=12)\n",
    "    plt.title('Effect of Learning Rate on Accuracy (CNN Model)', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.xticks(cnn_results['learning_rate'], [f'{x:.4f}' for x in cnn_results['learning_rate']])\n",
    "    \n",
    "    plt.subplot(4, 2, 4)\n",
    "    bars = plt.bar(cnn_results['learning_rate'], cnn_results['training_time'], width=0.3, color='salmon', edgecolor='darkred', alpha=0.7)\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1, f'{height:.1f}s', ha='center', va='bottom', fontsize=9)\n",
    "    plt.xlabel('Learning Rate (log scale)', fontsize=12)\n",
    "    plt.ylabel('Training Time (seconds)', fontsize=12)\n",
    "    plt.title('Effect of Learning Rate on Training Time (CNN Model)', fontsize=14)\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(cnn_results['learning_rate'], [f'{x:.4f}' for x in cnn_results['learning_rate']])\n",
    "    \n",
    "    plt.subplot(4, 2, 5)\n",
    "    lstm_results = results_df[results_df['model_type'] == 'LSTM']\n",
    "    plt.plot(lstm_results['dropout'], lstm_results['val_accuracy'], 'go-', linewidth=2, markersize=10)\n",
    "    for x, y in zip(lstm_results['dropout'], lstm_results['val_accuracy']):\n",
    "        plt.annotate(f'{y:.3f}', (x, y), textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=9, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    plt.xlabel('Dropout Rate', fontsize=12)\n",
    "    plt.ylabel('Validation Accuracy', fontsize=12)\n",
    "    plt.title('Effect of Dropout Rate on Accuracy (LSTM Model)', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.xticks(lstm_results['dropout'])\n",
    "    \n",
    "    plt.subplot(4, 2, 6)\n",
    "    attn_results = results_df[results_df['model_type'] == 'LSTM+Attention']\n",
    "    plt.plot(attn_results['hidden_dim'], attn_results['val_accuracy'], 'mo-', linewidth=2, markersize=10)\n",
    "    for x, y in zip(attn_results['hidden_dim'], attn_results['val_accuracy']):\n",
    "        plt.annotate(f'{y:.3f}', (x, y), textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=9, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    plt.xlabel('Hidden Dimension', fontsize=12)\n",
    "    plt.ylabel('Validation Accuracy', fontsize=12)\n",
    "    plt.title('Effect of Hidden Dimension on Accuracy (LSTM+Attention Model)', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.xticks(attn_results['hidden_dim'])\n",
    "    \n",
    "    plt.subplot(4, 2, 7)\n",
    "    model_comparison = results_df.drop_duplicates(subset=['model_type'], keep='last')\n",
    "    colors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']\n",
    "    bars = plt.bar(model_comparison['model_type'], model_comparison['val_accuracy'], color=colors, width=0.6, edgecolor='black', linewidth=1.5)\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{height:.3f}', ha='center', va='bottom', fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    plt.xlabel('Model Architecture', fontsize=12)\n",
    "    plt.ylabel('Validation Accuracy', fontsize=12)\n",
    "    plt.title('Comparison of Model Architectures (Best Configuration)', fontsize=14)\n",
    "    plt.ylim(0.5, max(model_comparison['val_accuracy']) + 0.1)\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    plt.subplot(4, 2, 8)\n",
    "    bars = plt.bar(model_comparison['model_type'], model_comparison['training_time'], color=colors, width=0.6, edgecolor='black', linewidth=1.5, alpha=0.7)\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1, f'{height:.1f}s', ha='center', va='bottom', fontsize=10)\n",
    "    plt.xlabel('Model Architecture', fontsize=12)\n",
    "    plt.ylabel('Training Time (seconds)', fontsize=12)\n",
    "    plt.title('Training Time Comparison (Best Configuration)', fontsize=14)\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout(pad=3.0)\n",
    "    plt.savefig('hyperparameter_experiments.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    best_configs = []\n",
    "    for model in results_df['model_type'].unique():\n",
    "        model_results = results_df[results_df['model_type'] == model]\n",
    "        best_row = model_results.loc[model_results['val_accuracy'].idxmax()]\n",
    "        best_configs.append(best_row)\n",
    "    best_df = pd.DataFrame(best_configs)\n",
    "    print(\"\\nBest Hyperparameter Configurations:\")\n",
    "    print(best_df[['model_type', 'embedding_dim', 'hidden_dim', 'learning_rate', 'dropout', 'val_accuracy', 'training_time']])\n",
    "    \n",
    "    if len(results_df['embedding_dim'].unique()) > 1 and len(results_df['dropout'].unique()) > 1:\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        combinations = []\n",
    "        for emb in sorted(results_df['embedding_dim'].unique()):\n",
    "            for drop in sorted(results_df['dropout'].unique()):\n",
    "                matching = results_df[(results_df['embedding_dim'] == emb) & (results_df['dropout'] == drop)]\n",
    "                if not matching.empty:\n",
    "                    combinations.append({\n",
    "                        'embedding_dim': emb,\n",
    "                        'dropout': drop,\n",
    "                        'val_accuracy': matching['val_accuracy'].values[0]\n",
    "                    })\n",
    "        if combinations:\n",
    "            combo_df = pd.DataFrame(combinations)\n",
    "            pivot_df = combo_df.pivot(index='dropout', columns='embedding_dim', values='val_accuracy')\n",
    "            plt.subplot(1, 2, 1)\n",
    "            sns.heatmap(pivot_df, annot=True, fmt='.3f', cmap='viridis', cbar_kws={'label': 'Validation Accuracy'})\n",
    "            plt.title('Effect of Embedding Dim + Dropout Combinations')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('hyperparameter_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "    return best_df\n",
    "\n",
    "def run_hyperparameter_study():\n",
    "    subset_size = 5000\n",
    "    np.random.seed(42)\n",
    "    train_indices = np.random.choice(len(small_train), subset_size, replace=False)\n",
    "    print(\"\\nPrecomputing sequences for hyperparameter experiments...\")\n",
    "    train_sequences, train_labels = precompute_sequences([small_train[int(i)] for i in train_indices], vocab, max_len=50)\n",
    "    test_indices = np.random.choice(len(small_test), subset_size//5, replace=False)\n",
    "    test_sequences, test_labels = precompute_sequences([small_test[int(i)] for i in test_indices], vocab, max_len=50)\n",
    "    train_dataset_hp = AmazonReviewsDataset(train_sequences, train_labels)\n",
    "    test_dataset_hp = AmazonReviewsDataset(test_sequences, test_labels)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    results_df = run_hyperparameter_experiments(train_dataset_hp, test_dataset_hp, vocab_size, device)\n",
    "    best_configs = visualize_hyperparameter_results(results_df)\n",
    "    return best_configs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a67d23-2050-4268-92e4-1b102e8de764",
   "metadata": {},
   "source": [
    "## 8. Results and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d660845-8c37-4b30-9e92-8e0f6ca5d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison(model_results):\n",
    "    model_names = list(model_results.keys())\n",
    "    metric_values = [model_results[model]['accuracy'] for model in model_names]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(model_names, metric_values, color=['#3498db', '#2ecc71', '#e74c3c', '#f39c12'])\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{height:.4f}',\n",
    "                 ha='center', va='bottom')\n",
    "    plt.title('Model Comparison - Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig('model_comparison_accuracy.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def plot_multi_metric_comparison(model_results):\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    model_names = list(model_results.keys())\n",
    "    data = []\n",
    "    for model in model_names:\n",
    "        for metric in metrics:\n",
    "            data.append({\n",
    "                'Model': model,\n",
    "                'Metric': metric,\n",
    "                'Value': model_results[model][metric]\n",
    "            })\n",
    "    df = pd.DataFrame(data)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    chart = sns.barplot(x='Model', y='Value', hue='Metric', data=df)\n",
    "    for i, p in enumerate(chart.patches):\n",
    "        chart.annotate(f'{p.get_height():.3f}', (p.get_x() + p.get_width() / 2., p.get_height() + 0.01),\n",
    "                      ha='center', va='bottom', fontsize=8)\n",
    "    plt.title('Model Comparison - Multiple Metrics')\n",
    "    plt.ylabel('Value')\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.legend(title='Metric')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig('model_comparison_multi_metric.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def visualize_attention(model, text, vocab, tokenizer=None, max_len=100, device='cpu'):\n",
    "    model.eval()\n",
    "    if tokenizer:\n",
    "        tokens = tokenizer(text)\n",
    "    else:\n",
    "        tokens = clean_text(text).split()\n",
    "    seq = [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n",
    "    if len(seq) < max_len:\n",
    "        effective_len = len(seq)\n",
    "        seq = seq + [vocab[\"<PAD>\"]] * (max_len - len(seq))\n",
    "    else:\n",
    "        effective_len = max_len\n",
    "        seq = seq[:max_len]\n",
    "    seq_tensor = torch.tensor([seq], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        _, attn_weights = model(seq_tensor)\n",
    "    attn_data = attn_weights[0, :effective_len, 0].cpu().numpy()\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    attn_df = pd.DataFrame({'token': tokens[:effective_len], 'attention': attn_data})\n",
    "    sns.barplot(x='token', y='attention', data=attn_df)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title('Attention Weights per Token')\n",
    "    plt.tight_layout()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    norm_weights = (attn_data - attn_data.min()) / (attn_data.max() - attn_data.min() + 1e-10)\n",
    "    y_positions = np.arange(effective_len)\n",
    "    plt.barh(y_positions, norm_weights, color='skyblue')\n",
    "    plt.yticks(y_positions, tokens[:effective_len])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title('Attention Heatmap')\n",
    "    plt.xlabel('Normalized Attention')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('attention_visualization.png', dpi=300)\n",
    "    plt.show()\n",
    "    print(\"Tokens with attention weights:\")\n",
    "    for token, weight in zip(tokens[:effective_len], attn_data):\n",
    "        print(f\"{token}: {weight:.4f}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d4fcc5-d5b8-41e3-aa47-1082eb5ed937",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64616e3f-d161-4762-b970-0b31e8059a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba_model(texts, model, vocab, max_seq_len, device):\n",
    "    \"\"\"\n",
    "    Convert a list of raw text strings to prediction probabilities using the model.\n",
    "    If a text becomes empty after cleaning, it assigns a default \"<UNK>\" token.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    processed = []\n",
    "    for text in texts:\n",
    "        tokens = clean_text(text).split()\n",
    "        if len(tokens) == 0:\n",
    "            tokens = [\"<UNK>\"]\n",
    "        seq = [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n",
    "        if len(seq) < max_seq_len:\n",
    "            seq = seq + [vocab[\"<PAD>\"]] * (max_seq_len - len(seq))\n",
    "        else:\n",
    "            seq = seq[:max_seq_len]\n",
    "        processed.append(seq)\n",
    "    processed_tensor = torch.tensor(processed, dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(processed_tensor)\n",
    "        if isinstance(outputs, tuple):  # for models that return a tuple (e.g., with attention)\n",
    "            outputs = outputs[0]\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "def explain_instance(model, raw_text, vocab, max_seq_len, device, class_names=[\"Negative\", \"Positive\"], num_features=10):\n",
    "    \"\"\"\n",
    "    Generate and display a LIME explanation for a given review.\n",
    "    \"\"\"\n",
    "    explainer = LimeTextExplainer(class_names=class_names)\n",
    "    predict_fn = lambda texts: predict_proba_model(texts, model, vocab, max_seq_len, device)\n",
    "    explanation = explainer.explain_instance(raw_text, predict_fn, num_features=num_features)\n",
    "    explanation.show_in_notebook(text=raw_text)\n",
    "    fig = explanation.as_pyplot_figure()\n",
    "    plt.title(\"LIME Explanation\")\n",
    "    plt.show()\n",
    "    return explanation\n",
    "\n",
    "def explain_prediction(model, text, vocab, max_seq_len, device='cpu', num_features=10, class_names=[\"Negative\", \"Positive\"]):\n",
    "    explainer = LimeTextExplainer(class_names=class_names)\n",
    "    predict_fn = lambda texts: predict_proba_model(texts, model, vocab, max_seq_len, device)\n",
    "    explanation = explainer.explain_instance(text, predict_fn, num_features=num_features)\n",
    "    probs = predict_fn([text])[0]\n",
    "    pred_class = class_names[1] if probs[1] >= 0.5 else class_names[0]\n",
    "    probability = probs[1] if pred_class == class_names[1] else probs[0]\n",
    "    return explanation, pred_class, probability\n",
    "\n",
    "def compare_all_models_lime(text_sample, trained_models, vocab, max_seq_len, device='cpu'):\n",
    "    \"\"\"Compare LIME explanations across all provided model architectures.\"\"\"\n",
    "    fig, axes = plt.subplots(len(trained_models), 2, figsize=(15, 5 * len(trained_models)))\n",
    "    model_names = list(trained_models.keys())\n",
    "    \n",
    "    for i, model_name in enumerate(model_names):\n",
    "        model = trained_models[model_name]\n",
    "        exp, class_name, probability = explain_prediction(model, text_sample, vocab, max_seq_len, device=device)\n",
    "        axes[i, 0].bar(['Negative', 'Positive'],\n",
    "                       [1 - probability, probability] if class_name == \"Positive\" else [probability, 1 - probability],\n",
    "                       color=['#3498db', '#e74c3c'])\n",
    "        axes[i, 0].set_title(f\"{model_name}: {class_name} ({probability:.2f})\")\n",
    "        axes[i, 0].set_ylim(0, 1.0)\n",
    "        \n",
    "        desired_label = 0 if class_name == \"Negative\" else 1\n",
    "        try:\n",
    "            words_weights = exp.as_list(label=desired_label)\n",
    "        except KeyError:\n",
    "            available_labels = list(exp.local_exp.keys())\n",
    "            if available_labels:\n",
    "                print(f\"Warning: Using label {available_labels[0]} instead of {desired_label} for {model_name}\")\n",
    "                words_weights = exp.as_list(label=available_labels[0])\n",
    "            else:\n",
    "                words_weights = []\n",
    "        \n",
    "        sorted_indices = np.argsort([abs(w[1]) for w in words_weights])[::-1][:10]\n",
    "        words = [words_weights[j][0] for j in sorted_indices]\n",
    "        weights = [words_weights[j][1] for j in sorted_indices]\n",
    "        colors = ['green' if w > 0 else 'red' for w in weights]\n",
    "        y_pos = np.arange(len(words))\n",
    "        axes[i, 1].barh(y_pos, weights, color=colors)\n",
    "        axes[i, 1].set_yticks(y_pos)\n",
    "        axes[i, 1].set_yticklabels(words)\n",
    "        axes[i, 1].set_title(f\"Top Words influencing {model_name} prediction\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_explanations_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def create_enhanced_lime_visualization(text, exp, class_idx, model_name):\n",
    "    \"\"\"Create an enhanced visualization for a LIME explanation.\"\"\"\n",
    "    words_weights = exp.as_list(label=class_idx)\n",
    "    tokens = text.lower().split()\n",
    "    word_data = []\n",
    "    for word in tokens:\n",
    "        weight = 0\n",
    "        for w, val in words_weights:\n",
    "            if w == word:\n",
    "                weight = val\n",
    "                break\n",
    "        word_data.append({\n",
    "            'word': word,\n",
    "            'weight': weight,\n",
    "            'color': 'green' if weight > 0 else 'red' if weight < 0 else 'gray',\n",
    "            'abs_weight': abs(weight)\n",
    "        })\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    words_df = pd.DataFrame(word_data)\n",
    "    top_words = words_df.sort_values('abs_weight', ascending=False).head(15)\n",
    "    colors = top_words['color']\n",
    "    plt.barh(range(len(top_words)), top_words['weight'], color=colors, alpha=0.8)\n",
    "    plt.yticks(range(len(top_words)), top_words['word'])\n",
    "    plt.xlabel('Weight / Influence')\n",
    "    plt.title(f'Top Influential Words - {model_name}')\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.axis('off')\n",
    "    colormap = plt.cm.RdYlGn\n",
    "    max_weight = max([abs(w['weight']) for w in word_data])\n",
    "    normalized_weights = [(w['weight'] / max_weight) if max_weight > 0 else 0 for w in word_data]\n",
    "    highlighted_text = \"\"\n",
    "    for i, word in enumerate(tokens):\n",
    "        weight = normalized_weights[i]\n",
    "        color_val = (weight + 1) / 2\n",
    "        color = colormap(color_val)\n",
    "        rgb = f'rgb({int(color[0]*255)},{int(color[1]*255)},{int(color[2]*255)})'\n",
    "        size = 10 + abs(weight) * 6\n",
    "        highlighted_text += f'<span style=\"background-color:{rgb}; font-size:{size}pt\">{word}</span> '\n",
    "    plt.text(0.5, 0.5, highlighted_text, fontsize=12, ha='center', va='center', wrap=True, transform=plt.gca().transAxes)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'enhanced_lime_{model_name.lower().replace(\" \", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def analyze_test_cases(test_reviews, trained_models, vocab, max_seq_len, device='cpu'):\n",
    "    for i, review in enumerate(test_reviews):\n",
    "        print(f\"\\n=== Test Case {i+1} ===\")\n",
    "        print(review)\n",
    "        compare_all_models_lime(review, trained_models, vocab, max_seq_len, device=device)\n",
    "        print(\"\\nAnalysis: (Add your detailed discussion here about model agreements/disagreements.)\\n\")\n",
    "\n",
    "def analyze_word_importance_patterns(test_corpus, models, vocab, max_seq_len, device='cpu'):\n",
    "    \"\"\"\n",
    "    Analyze which words are consistently important across different models.\n",
    "    \n",
    "    For each text in the test corpus, get the LIME explanation from each model.\n",
    "    If the desired label is not found in the explanation (KeyError), try the alternative label.\n",
    "    Then aggregate the importance scores across models and plot the top words.\n",
    "    \"\"\"\n",
    "    word_importance = {}\n",
    "    for text in test_corpus:\n",
    "        for model_name, model in models.items():\n",
    "            exp, class_name, _ = explain_prediction(model, text, vocab, max_seq_len, device=device)\n",
    "            class_idx = 0 if class_name == \"Negative\" else 1\n",
    "            try:\n",
    "                words_weights = exp.as_list(label=class_idx)\n",
    "            except KeyError:\n",
    "                alternative_label = 1 - class_idx\n",
    "                try:\n",
    "                    words_weights = exp.as_list(label=alternative_label)\n",
    "                    print(f\"Warning: Using alternative label {alternative_label} for model {model_name} on text: '{text[:50]}...'\")\n",
    "                except KeyError:\n",
    "                    print(f\"Warning: No label found for model {model_name} for text: '{text[:50]}...'\")\n",
    "                    words_weights = []\n",
    "            for word, weight in words_weights:\n",
    "                if word not in word_importance:\n",
    "                    word_importance[word] = {model_name: []}\n",
    "                elif model_name not in word_importance[word]:\n",
    "                    word_importance[word][model_name] = []\n",
    "                word_importance[word][model_name].append(weight)\n",
    "    aggregated_importance = {}\n",
    "    for word, model_scores in word_importance.items():\n",
    "        aggregated_importance[word] = {}\n",
    "        for model, weights in model_scores.items():\n",
    "            if weights:\n",
    "                aggregated_importance[word][model] = sum(weights) / len(weights)\n",
    "    importance_scores = []\n",
    "    for word, model_scores in aggregated_importance.items():\n",
    "        # Only include words that appeared for every model.\n",
    "        if len(model_scores) == len(models):\n",
    "            avg_score = sum(model_scores.values()) / len(model_scores)\n",
    "            std_score = np.std(list(model_scores.values()))\n",
    "            importance_scores.append((word, avg_score, std_score))\n",
    "    importance_scores.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    top_words = importance_scores[:20]\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    words = [w[0] for w in top_words]\n",
    "    scores = [w[1] for w in top_words]\n",
    "    errors = [w[2] for w in top_words]\n",
    "    colors = ['green' if s > 0 else 'red' for s in scores]\n",
    "    y_pos = np.arange(len(words))\n",
    "    plt.barh(y_pos, scores, xerr=errors, color=colors, alpha=0.7)\n",
    "    plt.yticks(y_pos, words)\n",
    "    plt.xlabel('Average Importance Score (with std dev)')\n",
    "    plt.title('Top Important Words Across All Models')\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('word_importance_patterns.png', dpi=300)\n",
    "    plt.show()\n",
    "    return aggregated_importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4225f89b-c899-435f-ae51-b65230a971bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "## 9. Conclusion\n",
    "\n",
    "In this project, we built and optimized multiple sentiment analysis models using a subset of the Amazon Reviews dataset. We covered data preprocessing, vocabulary building, and implemented four architectures: a Bag-of-Words + MLP, an LSTM, a CNN, and an LSTM with an attention mechanism. Hyperparameter experimentation allowed us to analyze the effects of embedding dimensions, learning rates, dropout rates, and hidden dimensions on model performance. Detailed evaluations with various metrics and visualizations provided insights into each model’s trade-offs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c10317-abb0-4aa8-9790-1804e73f6f41",
   "metadata": {},
   "source": [
    "\n",
    "## 10. References\n",
    "\n",
    "- Hugging Face Datasets: [https://huggingface.co/docs/datasets](https://huggingface.co/docs/datasets)\n",
    "- PyTorch Documentation: [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)\n",
    "- Devlin, J., et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. *arXiv preprint arXiv:1810.04805*.\n",
    "- Additional relevant sources as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ce8df-cf75-4ea3-8aea-264f44481c95",
   "metadata": {},
   "source": [
    "\n",
    "## Final Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac4f815-26b3-4879-b1b0-e09010c53b2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n====== Starting Main Experiments ======\")\n",
    "    # Run your main experiments (this should define and train your models)\n",
    "    def run_experiments(fast_mode=True):\n",
    "        embed_dim = 64 if fast_mode else 128\n",
    "        hidden_dim = 64 if fast_mode else 128\n",
    "        num_layers = 1\n",
    "        num_classes = 2\n",
    "        num_epochs = 3 if fast_mode else 5\n",
    "        batch_size = 128\n",
    "        max_seq_len_local = 50 if fast_mode else 100\n",
    "        print(\"Precomputing training sequences...\")\n",
    "        train_sequences, train_labels = precompute_sequences(small_train, vocab, max_len=max_seq_len_local)\n",
    "        print(\"Precomputing test sequences...\")\n",
    "        test_sequences, test_labels = precompute_sequences(small_test, vocab, max_len=max_seq_len_local)\n",
    "        train_dataset_main = AmazonReviewsDataset(train_sequences, train_labels)\n",
    "        test_dataset_main = AmazonReviewsDataset(test_sequences, test_labels)\n",
    "        num_workers = 0\n",
    "        train_loader_main = DataLoader(train_dataset_main, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "        test_loader_main = DataLoader(test_dataset_main, batch_size=batch_size, num_workers=num_workers, pin_memory=True)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        train_bow = True\n",
    "        train_lstm = True\n",
    "        train_cnn = True\n",
    "        train_attn = True\n",
    "        if fast_mode:\n",
    "            subset_size = 10000\n",
    "            print(f\"Using a small subset of {subset_size} examples for extra fast mode\")\n",
    "            subset_indices = torch.randperm(len(train_dataset_main))[:subset_size]\n",
    "            subset_dataset = torch.utils.data.Subset(train_dataset_main, subset_indices)\n",
    "            train_loader_main = DataLoader(subset_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "        model_results = {}\n",
    "        train_loss_dict = {}\n",
    "        val_loss_dict = {}\n",
    "        if train_bow:\n",
    "            print(\"\\n=== Training Bag-of-Words + MLP model ===\")\n",
    "            bow_model = BoWModel(vocab_size, embed_dim, hidden_dim, num_classes)\n",
    "            bow_train_losses, bow_val_losses = train_model(bow_model, train_loader_main, test_loader_main, num_epochs, lr=1e-3, device=device)\n",
    "            train_loss_dict['BoW'] = bow_train_losses\n",
    "            val_loss_dict['BoW'] = bow_val_losses\n",
    "            bow_results = evaluate_model(bow_model, test_loader_main, device=device)\n",
    "            model_results['BoW'] = bow_results\n",
    "        if train_lstm:\n",
    "            print(\"\\n=== Training LSTM model ===\")\n",
    "            lstm_model = LSTMModel(vocab_size, embed_dim, hidden_dim, num_layers, num_classes, bidirectional=True)\n",
    "            lstm_train_losses, lstm_val_losses = train_model(lstm_model, train_loader_main, test_loader_main, num_epochs, lr=1e-3, device=device)\n",
    "            train_loss_dict['LSTM'] = lstm_train_losses\n",
    "            val_loss_dict['LSTM'] = lstm_val_losses\n",
    "            lstm_results = evaluate_model(lstm_model, test_loader_main, device=device)\n",
    "            model_results['LSTM'] = lstm_results\n",
    "        if train_cnn:\n",
    "            print(\"\\n=== Training CNN model ===\")\n",
    "            cnn_model = CNNModel(vocab_size, embed_dim, num_classes)\n",
    "            cnn_train_losses, cnn_val_losses = train_model(cnn_model, train_loader_main, test_loader_main, num_epochs, lr=1e-3, device=device)\n",
    "            train_loss_dict['CNN'] = cnn_train_losses\n",
    "            val_loss_dict['CNN'] = cnn_val_losses\n",
    "            cnn_results = evaluate_model(cnn_model, test_loader_main, device=device)\n",
    "            model_results['CNN'] = cnn_results\n",
    "        if train_attn:\n",
    "            print(\"\\n=== Training LSTM with Attention model ===\")\n",
    "            attn_model = AttnLSTMModel(vocab_size, embed_dim, hidden_dim, num_layers, num_classes, bidirectional=True)\n",
    "            attn_train_losses, attn_val_losses = train_model(attn_model, train_loader_main, test_loader_main, num_epochs, lr=1e-3, use_attention=True, device=device)\n",
    "            train_loss_dict['LSTM+Attention'] = attn_train_losses\n",
    "            val_loss_dict['LSTM+Attention'] = attn_val_losses\n",
    "            attn_results = evaluate_model(attn_model, test_loader_main, device=device, use_attention=True)\n",
    "            model_results['LSTM+Attention'] = attn_results\n",
    "        if len(model_results) > 1:\n",
    "            print(\"\\n=== Model Comparison ===\")\n",
    "            plot_model_comparison(model_results)\n",
    "            plot_multi_metric_comparison(model_results)\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            epochs_range = range(1, num_epochs + 1)\n",
    "            for model_name, losses in train_loss_dict.items():\n",
    "                plt.plot(epochs_range, losses, 'o-', label=model_name)\n",
    "            plt.title('Training Loss Comparison')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.subplot(1, 2, 2)\n",
    "            for model_name, losses in val_loss_dict.items():\n",
    "                plt.plot(epochs_range, losses, 'o-', label=model_name)\n",
    "            plt.title('Validation Loss Comparison')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('model_comparison_learning_curves.png', dpi=300)\n",
    "            plt.show()\n",
    "        return model_results\n",
    "\n",
    "    print(\"\\n====== Starting Main Experiments ======\")\n",
    "    model_results = run_experiments(fast_mode=True)\n",
    "    \n",
    "    print(\"\\n====== Starting Hyperparameter Experimentation ======\")\n",
    "    best_configs = run_hyperparameter_study()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea76dca1-8839-4d6c-a4e0-0ea6e46c2ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n====== Model Explainability with LIME ======\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the BoW model from file (adjust filename as needed)\n",
    "bow_model_explain = BoWModel(vocab_size, 64, 64, num_classes=2)\n",
    "if os.path.exists(\"best_bowmodel_model.pt\"):\n",
    "    bow_model_explain.load_state_dict(torch.load(\"best_bowmodel_model.pt\", map_location=device))\n",
    "else:\n",
    "    print(\"Warning: best_bowmodel_model.pt not found. Using untrained BoW model.\")\n",
    "bow_model_explain = bow_model_explain.to(device)\n",
    "\n",
    "# Load the LSTM model from file (if available)\n",
    "lstm_model_explain = LSTMModel(vocab_size, 64, 64, num_layers=1, num_classes=2, bidirectional=True)\n",
    "if os.path.exists(\"best_lstmmodel_model.pt\"):\n",
    "    lstm_model_explain.load_state_dict(torch.load(\"best_lstmmodel_model.pt\", map_location=device))\n",
    "else:\n",
    "    print(\"Warning: best_lstm_model.pt not found. Using untrained LSTM model.\")\n",
    "lstm_model_explain = lstm_model_explain.to(device)\n",
    "\n",
    "# Load the CNN model from file (if available)\n",
    "cnn_model_explain = CNNModel(vocab_size, 64, num_classes=2, dropout=0.2)\n",
    "if os.path.exists(\"best_cnnmodel_model.pt\"):\n",
    "    cnn_model_explain.load_state_dict(torch.load(\"best_cnnmodel_model.pt\", map_location=device))\n",
    "else:\n",
    "    print(\"Warning: best_cnn_model.pt not found. Using untrained CNN model.\")\n",
    "cnn_model_explain = cnn_model_explain.to(device)\n",
    "\n",
    "# Load the LSTM+Attention model from file\n",
    "attn_model_explain = AttnLSTMModel(vocab_size, 64, 64, num_layers=1, num_classes=2, bidirectional=True)\n",
    "if os.path.exists(\"best_attnlstmmodel_model.pt\"):\n",
    "    attn_model_explain.load_state_dict(torch.load(\"best_attnlstmmodel_model.pt\", map_location=device))\n",
    "else:\n",
    "    print(\"Warning: best_attnlstmmodel_model.pt not found. Using untrained LSTM+Attention model.\")\n",
    "attn_model_explain = attn_model_explain.to(device)\n",
    "\n",
    "# Define dictionary of all trained (or loaded) models for comparison.\n",
    "trained_models = {\n",
    "    \"BoW Model\": bow_model_explain,\n",
    "    \"LSTM Model\": lstm_model_explain,\n",
    "    \"CNN Model\": cnn_model_explain,\n",
    "    \"LSTM+Attention Model\": attn_model_explain\n",
    "}\n",
    "\n",
    "# Choose a sample review from the test set for explanation.\n",
    "sample_review = small_test[0]['content']\n",
    "print(\"Sample Review for Explanation:\")\n",
    "print(sample_review)\n",
    "\n",
    "# Generate LIME explanation for the BoW model.\n",
    "explanation_bow = explain_instance(bow_model_explain, sample_review, vocab, max_seq_len, device)\n",
    "\n",
    "# Generate LIME explanation for the LSTM+Attention model.\n",
    "explanation_attn = explain_instance(attn_model_explain, sample_review, vocab, max_seq_len, device)\n",
    "\n",
    "# ------------------------------\n",
    "# Comparative LIME Analysis Across Models\n",
    "# ------------------------------\n",
    "print(\"\\n====== Comparative LIME Analysis Across Models ======\")\n",
    "compare_all_models_lime(sample_review, trained_models, vocab, max_seq_len, device)\n",
    "\n",
    "# ------------------------------\n",
    "# Analyze Challenging Test Cases\n",
    "# ------------------------------\n",
    "print(\"\\n====== Analyzing Challenging Test Cases ======\")\n",
    "test_reviews = [\n",
    "    \"This product is amazing! I absolutely love it and would recommend to everyone.\",\n",
    "    \"Terrible product. Broke after two days and customer service was awful.\",\n",
    "    \"The product has some good features but overall I'm disappointed with its performance. The battery life is excellent, but the software is buggy.\",\n",
    "    \"I was expecting more from this product given its price point. It functions adequately.\",\n",
    "    \"Oh great, another product that breaks after a week. Just what I needed!\"\n",
    "]\n",
    "analyze_test_cases(test_reviews, trained_models, vocab, max_seq_len, device)\n",
    "\n",
    "# ------------------------------\n",
    "# Enhanced LIME Visualization for BoW Model\n",
    "# ------------------------------\n",
    "print(\"\\n====== Enhanced LIME Visualization for BoW Model ======\")\n",
    "exp_bow = explain_instance(trained_models[\"BoW Model\"], sample_review, vocab, max_seq_len, device)\n",
    "# Adjust class index based on prediction (here we assume positive, so class_idx=1)\n",
    "create_enhanced_lime_visualization(sample_review, exp_bow, class_idx=1, model_name=\"BoW Model\")\n",
    "\n",
    "# ------------------------------\n",
    "# Analyze Overall Word Importance Patterns Across Models\n",
    "# ------------------------------\n",
    "print(\"\\n====== Analyzing Word Importance Patterns ======\")\n",
    "test_corpus = [small_test[i]['content'] for i in range(10)]\n",
    "word_importance_patterns = analyze_word_importance_patterns(test_corpus, trained_models, vocab, max_seq_len, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d723e64c-0b68-41a5-a3a2-879f8ca91ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seems like the files are saved as \n",
    "best_lstmmodel_model.pt and \n",
    "best_cnnmodel.pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
